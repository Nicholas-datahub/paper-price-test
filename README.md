# ğŸ“ˆ Paper Price Test - Data Engineer Case

Projeto de coleta automatizada de indicadores econÃ´micos pÃºblicos para previsÃ£o do preÃ§o do papel. Desenvolvido em Python com deploy na Google Cloud Platform.

## ğŸ§  Objetivo

Extrair diariamente 3 indicadores do site [investing.com](https://br.investing.com):

- **Chinese Caixin Services Index**
- **Bloomberg Commodity Index**
- **USD/CNY**

Esses dados sÃ£o salvos em formato tabular no Google Cloud Storage e prontos para ingestÃ£o no BigQuery.

---

## ğŸ› ï¸ Tecnologias Utilizadas

- **Python 3.10** + Selenium + BeautifulSoup + Pandas
- **Google Cloud Platform (GCP)**
  - Cloud Storage (GCS)
  - BigQuery
  - Cloud Run
  - Cloud Composer (Airflow)
- **Terraform** para infraestrutura como cÃ³digo
- **Docker** para conteinerizaÃ§Ã£o

---

## ğŸ“‚ Estrutura do Projeto

```bash
paper-price-test/
â”œâ”€â”€ dags/                       # DAG do Airflow (Cloud Composer)
â”‚   â””â”€â”€ invest_data_pipeline.py
â”œâ”€â”€ scraper/                   # Scripts de scraping
â”‚   â””â”€â”€ main_scraper.py
â”œâ”€â”€ terraform/                 # IaC para GCP
â”‚   â””â”€â”€ main.tf
â”œâ”€â”€ Dockerfile                 # Empacota o scraper para Cloud Run
â”œâ”€â”€ requirements.txt           # Bibliotecas Python
â””â”€â”€ README.md                  # DocumentaÃ§Ã£o do projeto
```

---

## âš™ï¸ Setup Local

### 1. Clone o repositÃ³rio
```bash
git clone https://github.com/SEU_USUARIO/paper-price-test.git
cd paper-price-test
```

### 2. Instale dependÃªncias (opcional com virtualenv)
```bash
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 3. Exporte variÃ¡veis de ambiente (GCP)
```bash
export GOOGLE_APPLICATION_CREDENTIALS="path/to/key.json"
export GCS_BUCKET="investing-data-bucket"
```

### 4. Execute localmente
```bash
python scraper/main_scraper.py
```

Os arquivos `caixin_index.csv`, `bloomberg_commodity_index.csv` e `usd_cny.csv` serÃ£o enviados para o bucket.

---

## ğŸ³ Deploy via Docker

### Build da imagem
```bash
docker build -t paper-scraper .
```

### ExecuÃ§Ã£o local
```bash
docker run -e GCS_BUCKET=investing-data-bucket -v $HOME/.config/gcloud:/root/.config/gcloud paper-scraper
```

---

## â˜ï¸ Deploy na Google Cloud

### 1. Enviar imagem para o Container Registry
```bash
gcloud builds submit --tag gcr.io/paper-price/scraper
```

### 2. Deploy com Terraform
```bash
cd terraform
terraform init
terraform apply
```

Recursos provisionados:
- Bucket GCS
- Dataset BigQuery
- Cloud Run Service
- Cloud Composer (Airflow DAG)

---

## ğŸ›« Agendamento DiÃ¡rio

O Airflow (Cloud Composer) executa a DAG `investing_data_pipeline` todos os dias Ã s 6h (UTC), chamando o script principal para scraping e salvamento no GCS.

---

## ğŸ“Œ ObservaÃ§Ãµes
- O scraping depende da estrutura do HTML do investing.com e pode quebrar com mudanÃ§as no site.
- A raspagem utiliza Chromium headless e pode demandar ajustes de timeout.

---

## ğŸ‘¨â€ğŸ’» Autor

Desenvolvido por **Lucas Nicholas** para desafio tÃ©cnico de engenharia de dados.

ğŸ“§ [linkedin.com/in/lucas-nicholas](https://linkedin.com/in/lucas-nicholas)

---

## âœ… Checklist de Entrega

- [x] Scraper funcional (Caixin, Bloomberg, USD/CNY)
- [x] Upload para GCS
- [x] Deploy via Docker e Cloud Run
- [x] DAG Airflow para Composer
- [x] Terraform com todos recursos
- [x] CÃ³digo versionado no GitHub privado
